{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-kamireddy/Logistic-Regression-for-Heart-Attack-Risk-Predictiom/blob/main/projects_in_ai_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries from titanic example\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "7svOsVyIu86j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5_KSRyPnZPdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1:\n",
        "1. Derive the objective function for Logistic Regression using Maximum Likelihood\n",
        "Estimation (MLE). Do some research on the MAP technique for Logistic Regression,\n",
        "include your research on how this technique is different from MLE (include citations).\n",
        "\n",
        "  >Deriving the logit function:\n",
        "\n",
        "  Starting with the liklihood function:\n",
        "\n",
        "  $L(w) = Π_{y_i = 1}(p(x^i)) * Π_{y_i = 1}(1-p(x^i)) $\n",
        "\n",
        "  $= Π(p(x^i)^{y^i} + (1-p(x^i))^{1-y^i})$\n",
        "\n",
        "  Our goal is to minimize this function, the minimum of the log will occur at the same place so we can take the logarithm and divide by the number of samples:\n",
        "\n",
        "  $l(w) =\\frac{1}{n} log(Π(p(x^i)^{y^i} + (1-p(x^i))^{1-y^i}))$\n",
        "\n",
        "  $ = \\frac{1}{n}Σ( y^i log(p(x^i)) + (1-y^i)log(1-p(x^i)) )$\n",
        "\n",
        "  $p(x^i) = \\frac{1}{1+ e^{-wx^i}} $\n",
        "\n",
        "  $l(w) = \\frac{1}{n}Σ( y^i log( \\frac{1}{1+ e^{-wx^i}}) + (1-y^i)log(1-  \\frac{1}{1+ e^{-wx^i}}) )$\n",
        "\n",
        "  $ = \\frac{1}{n}Σ( y^i log( \\frac{1}{1+ e^{-wx^i}}) + (1-y^i)log( \\frac{e^{-wx^i}}{1+ e^{-wx^i}}) )$\n",
        "\n",
        "  $= \\frac{1}{n}Σ( y^i (log( \\frac{1}{1+ e^{-wx^i}}) - log(\\frac{e^{-wx^i}}{1+ e^{-wx^i}}) ) + log( \\frac{e^{-wx^i}}{1+ e^{-wx^i}}) )$\n",
        "  $= \\frac{1}{n}Σ( y^i (log( \\frac{1}{1+ e^{-wx^i}}) - log(\\frac{e^{-wx^i}}{1+ e^{-wx^i}}) ) ) +  \\frac{1}{n}log( \\frac{e^{-wx^i}}{1+ e^{-wx^i}})  $\n",
        "  $ =  \\frac{1}{n}Σ( y^i (log( \\frac{1}{1+ e^{-wx^i}}) - log(\\frac{1}{1+ e^{-wx^i}}) + log(e^{-wx^i})) ) +  \\frac{1}{n}log( \\frac{e^{-wx^i}}{1+ e^{-wx^i}}) = \\frac{1}{n}Σ( y^i ( log(e^{-wx^i})) ) +  \\frac{1}{n}log( \\frac{e^{-wx^i}}{1+ e^{-wx^i}})  $\n",
        "  $=  \\frac{1}{n}Σ( -y^iwx^i) +  \\frac{1}{n}log( \\frac{e^{-wx^i}}{1+ e^{-wx^i}})$\n",
        "\n",
        "  which is our objective function.\n",
        "\n",
        "  The MAP or Maximum a Postori estimate is different from MLE, because where MLE only considers the liklihood of the sample data, MAP considers both the liklihood of data as well as a prior belief distribution, with the goal being to find the most likley model parameters given the data, rather than minimize a loss function.\n",
        "\n",
        "  Source: https://www.cs.cornell.edu/courses/cs4780/2015fa/web/lecturenotes/lecturenote06.html\n",
        "\n",
        "2. Define a machine learning problem you wish to solve using Logistic Regression. Justify\n",
        "why logistic regression is the best choice and compare it briefly to another linear\n",
        "classification model (cite your work if this other technique was not covered in class).\n",
        "\n",
        "The machine learning problem I would like to solve using logistic regression is the liklihood of a heart attack based on a number of demographic and lifestyle factors. This is because the values are binary (heart attack vs. no heart attack) and we want a liklihood as the output of learning (between 0 and 1) making logistic regression a good choice compared to something like linear regression can output any number in the space of real numbers.\n",
        "\n",
        "3. Discuss how your dataset corresponds to the variables in your equations, highlighting\n",
        "any assumptions in your derivation from part 1.\n",
        "\n",
        "Assuming x is [1, x_1, x_2, ... , x_d] and w is of the form [w_0, w_1, w_2, ..., w_d] (where w_0 is the bias and the rest of the w values correspond to weights). $x^i$ corresponds to the lifestyle factors of each individual\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Q0biHENN0H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2:\n",
        "----\n",
        "1. Link to dataset: https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset\n"
      ],
      "metadata": {
        "id": "56VRB33JsRlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. EDA\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"iamsouravbanerjee/heart-attack-prediction-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "df = pd.read_csv(path +'/heart_attack_prediction_dataset.csv')"
      ],
      "metadata": {
        "id": "fwzfSU-ntfB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Dictionary**\n",
        "\n"
      ],
      "metadata": {
        "id": "YNeNS-_w7g6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "MXN92d9n65Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n",
        "df.isna().sum()\n"
      ],
      "metadata": {
        "id": "_KDghseQ7snO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Q9S2XvmFK7eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe() #dataset stats"
      ],
      "metadata": {
        "id": "PK6bRxzLLRWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=['O'])"
      ],
      "metadata": {
        "id": "joNtcF1ILWRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Heart Attack Risk\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "h2Kr3WYULl46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert categorical data to numerical data using cat.codes\n",
        "df['Sex'] = df['Sex'].astype('category')\n",
        "df['Sex'] = df['Sex'].cat.codes\n",
        "df['Exercise Hours Per Week'] = df['Exercise Hours Per Week'].astype('category')\n",
        "df['Exercise Hours Per Week'] = df['Exercise Hours Per Week'].cat.codes\n",
        "df['Diet'] = df['Diet'].astype('category')\n",
        "df['Diet'] = df['Diet'].cat.codes\n",
        "df['Country'] = df['Country'].astype('category')\n",
        "df['Country'] = df['Country'].cat.codes\n",
        "df['Continent'] = df['Continent'].astype('category')\n",
        "df['Continent'] = df['Continent'].cat.codes\n",
        "df['Hemisphere'] = df['Hemisphere'].astype('category')\n",
        "df['Hemisphere'] = df['Hemisphere'].cat.codes\n",
        "\n",
        "#convert blood pressure to a simple numeric value using Mean Arterial Blood pressure formula\n",
        "def bp_value(str):\n",
        "  str = str.split('/')\n",
        "  return (float(str[0])+ (2*float(str[1])))/3\n",
        "\n",
        "df['Blood Pressure'] = df['Blood Pressure'].map(bp_value)\n",
        "\n",
        "#remove patient ID\n",
        "df.drop('Patient ID', axis=1, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "PXB8KsznXG2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Arterial Blood Pressure formula source: https://clinicalview.gehealthcare.com/white-paper/measuring-mean-arterial-pressure-choosing-most-accurate-method"
      ],
      "metadata": {
        "id": "yT-wD-8fiHhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "RnXdN6XuezHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show distributions\n",
        "df.hist(figsize=(15,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yv9ILN8VMIE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#colinearity\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = df.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i)\n",
        "                          for i in range(len(df.columns))]\n",
        "print(vif_data)\n",
        "#high vif values indicate colinearity\n",
        "\n",
        "#check which variables are colinear with the covariance matrix\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z6Qb1mjZmK2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see high rates of correlation between country, hemisphere, and continent (as expected) but also between age, sex, and smoking (presumably because older people and males are more likley to smoke). Therefore we will remove the hemisphere, continent, and smoking features in this dataset."
      ],
      "metadata": {
        "id": "nW6xzexZq_9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Continent', 'Hemisphere', 'Smoking'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "veJwiewUrl7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardizing the data\n",
        "for column in df.columns:\n",
        "    df[column] = (df[column] -\n",
        "                           df[column].mean()) / df[column].std()"
      ],
      "metadata": {
        "id": "iRTIEPdM8QgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3\n",
        "----\n",
        "Now we implement the cost function, as well as the three types of vanilla SGD (batch, minibatch, and stochastic)"
      ],
      "metadata": {
        "id": "B_RAzCulgoCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit_cost(X, Y, y_hat, w):#cost function\n",
        "  return -torch.mean(Y * torch.log(y_hat) + (1 - Y) * torch.log(1 - y_hat))\n",
        "\n"
      ],
      "metadata": {
        "id": "Lwv7VZ8md4O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent_batch( X, Y ):\n",
        "  losses= []\n",
        "  w = torch.zeros(X.shape[1], dtype = torch.float64, requires_grad=True)\n",
        "\n",
        "  for _ in range(10000):\n",
        "    z = torch.matmul(X, w)\n",
        "    y_hat = 1 / (1 + torch.exp(-z))\n",
        "    loss = logit_cost(X, Y, y_hat,w)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    gradient = torch.matmul(X.T, (y_hat - Y)) / X.shape[0]\n",
        "    w = w - (gradient * .001)\n",
        "\n",
        "  print(losses)\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  return w , losses\n"
      ],
      "metadata": {
        "id": "0doYZS0SUeFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_minibatch( X, Y ):\n",
        "  losses= []\n",
        "  w = torch.zeros(X.shape[1], dtype = torch.float64, requires_grad=True)\n",
        "  xbatches = []\n",
        "  ybatches = []\n",
        "  for i in range(0, 69):\n",
        "    xbatches.append(X[i:i+127])\n",
        "    ybatches.append(Y[i:i+127])\n",
        "\n",
        "\n",
        "  for i in range(10000):\n",
        "    x_batch = xbatches[i % len(xbatches)]\n",
        "    y_batch = ybatches[i % len(ybatches)]\n",
        "    z = torch.matmul(x_batch, w)\n",
        "    y_hat = 1 / (1 + torch.exp(-z))\n",
        "    loss = logit_cost(x_batch, y_batch, y_hat, w)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    gradient = torch.matmul(x_batch.T, (y_hat - y_batch)) / x_batch.shape[0]\n",
        "    w = w - (gradient * .001)\n",
        "\n",
        "  print(losses)\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  return w , losses"
      ],
      "metadata": {
        "id": "-wdPXkHuigiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_stochastic( X, Y ):\n",
        "  losses= []\n",
        "  w = torch.zeros(X.shape[1], dtype = torch.float64, requires_grad=True)\n",
        "\n",
        "  for _ in range(1000):\n",
        "    x = X[np.random.randint(0, X.shape[0])]\n",
        "    y = Y[np.random.randint(0, Y.shape[0])]\n",
        "    z = torch.matmul(x, w)\n",
        "    y_hat = 1 / (1 + torch.exp(-z))\n",
        "    loss = logit_cost(x, y, y_hat, w)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    gradient = torch.matmul(x.T, (y_hat - y)) / x.shape[0]\n",
        "    w = w - (gradient * .01)\n",
        "\n",
        "  print(losses)\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  return w , losses"
      ],
      "metadata": {
        "id": "I5nTpFna3i4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def test_error(X, Y, W):\n",
        "  "
      ],
      "metadata": {
        "id": "2Eu3aMrLoQU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_error(X, Y, W):\n",
        "  z = torch.matmul(X, W)\n",
        "  y_hat = 1 / (1 + torch.exp(-z))\n",
        "\n",
        "  error = torch.mean(torch.abs(y_hat - Y))\n",
        "  return error"
      ],
      "metadata": {
        "id": "jQhc3nqgohZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['Heart Attack Risk'].values\n",
        "X = df.drop('Heart Attack Risk', axis=1).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "wW6LqTr7jjJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(X_train)\n",
        "x_train = torch.cat((torch.ones(x_train.shape[0], 1), x_train), dim=1)\n",
        "y_train = torch.tensor(Y_train, dtype = torch.float64)\n",
        "x_test = torch.tensor(X_test)\n",
        "x_test = torch.cat((torch.ones(x_test.shape[0], 1), x_test), dim=1)\n",
        "y_test = torch.tensor(Y_test, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "qskMJwcVju11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.dtype"
      ],
      "metadata": {
        "id": "Pp3KcllMePVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, loss = gradient_descent_batch(   x_train, y_train)\n",
        "print(test_error(x_test, y_test, w))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7fbj8vERdgU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, loss = gradient_descent_minibatch(   x_train, y_train)\n",
        "print(test_error(x_test, y_test, w))\n",
        "\n"
      ],
      "metadata": {
        "id": "1lBUSg-m1BL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, loss = gradient_descent_stochastic(   x_train, y_train)\n",
        "print(test_error(x_test, y_test, w))\n"
      ],
      "metadata": {
        "id": "BKR_dxJE40aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Optimization Techniques and Advanced Comparision"
      ],
      "metadata": {
        "id": "chp3-AFL43ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_stochastic_momentum( X, Y ):\n",
        "  losses = []\n",
        "  w = torch.zeros(X.shape[1], dtype = torch.float64, requires_grad=True)\n",
        "  v_w = torch.zeros(X.shape[1], dtype = torch.float64, requires_grad=True)\n",
        "\n",
        "  alpha = 0.01\n",
        "  beta = 0.9\n",
        "  v = 0\n",
        "\n",
        "  for _ in range(100000):\n",
        "    x = X[np.random.randint(0, X.shape[0])]\n",
        "    y = Y[np.random.randint(0, Y.shape[0])]\n",
        "    z = torch.matmul(x, w)\n",
        "    y_hat = logit_cost(x, y, y_hat, w)\n",
        "    loss = logit_cost(x, y, y_hat, w)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    gradient = torch.matmul(x.T, (y_hat - y)) / x.shape[0]\n",
        "\n",
        "    v = beta * v + (1 - beta) * gradient\n",
        "    w = w - (alpha * v)\n",
        "\n",
        "  print(losses)\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  return w , losses\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YvAXnw9H_8Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS8-y53WZmrK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}